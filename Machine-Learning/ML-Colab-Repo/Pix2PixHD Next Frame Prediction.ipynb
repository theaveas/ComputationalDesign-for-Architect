{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pix2PixHD Next Frame Prediction.ipynb","provenance":[{"file_id":"https://github.com/dvschultz/ml-art-colabs/blob/master/Pix2PixHD_Next_Frame_Prediction.ipynb","timestamp":1623650399184}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"VXoBaXRDKuMV"},"source":["#Next Frame Prediction using Pix2PixHD\n","\n","By Derrick Schultz\n","\n","Forked repo and tutorial based on [JC Testud’s excellent repo and Medium](https://medium.com/@jctestud/video-generation-with-pix2pix-aed5b1b69f57) article."]},{"cell_type":"markdown","metadata":{"id":"xfOexYWJX3Pt"},"source":["## Mount Google Drive"]},{"cell_type":"code","metadata":{"id":"K--AsrIzpH58","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623657565774,"user_tz":-480,"elapsed":29268,"user":{"displayName":"Theaveas So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN_h8gjOMXrtWaxBWxEbW87dJL1IF6Rtq-iYc6=s64","userId":"12302225738999643817"}},"outputId":"4e5df958-038c-4dcd-cd70-d5dc718bb316"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o9D_7iUQINoa"},"source":["Check to see what GPU we’re assigned"]},{"cell_type":"code","metadata":{"id":"ABG3hL4lIQGP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623657766886,"user_tz":-480,"elapsed":407,"user":{"displayName":"Theaveas So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN_h8gjOMXrtWaxBWxEbW87dJL1IF6Rtq-iYc6=s64","userId":"12302225738999643817"}},"outputId":"88bf128e-ac5a-4e9d-e6ef-b136887d602f"},"source":["!nvidia-smi -L"],"execution_count":7,"outputs":[{"output_type":"stream","text":["GPU 0: Tesla T4 (UUID: GPU-12215951-5394-74ae-b203-d00f5e178325)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5oDni8bLhrov","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623657580847,"user_tz":-480,"elapsed":7,"user":{"displayName":"Theaveas So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN_h8gjOMXrtWaxBWxEbW87dJL1IF6Rtq-iYc6=s64","userId":"12302225738999643817"}},"outputId":"6bc7f410-87ec-4658-c24c-a2d084c3ba33"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cUvLbCJtLqaV"},"source":["## Install libraries and dependencies\n","\n"]},{"cell_type":"code","metadata":{"id":"bQbRsmWdvjUo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623657595487,"user_tz":-480,"elapsed":4349,"user":{"displayName":"Theaveas So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN_h8gjOMXrtWaxBWxEbW87dJL1IF6Rtq-iYc6=s64","userId":"12302225738999643817"}},"outputId":"dcbb14ba-0f1e-4c2d-87f2-1e91a498ebb3"},"source":["import os\n","if os.path.isdir(\"/content/drive/MyDrive/nfp-colab/pix2pixHD/\"):\n","    %cd /content/drive/MyDrive/nfp-colab/pix2pixHD/\n","    # !git pull\n","    !pip install dominate\n","else:\n","    %cd /content/drive/MyDrive\n","    !mkdir nfp-colab\n","    %cd nfp-colab\n","    !git clone -b video https://github.com/dvschultz/pix2pixHD.git\n","    !pip install dominate\n","    %cd pix2pixHD/"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/nfp-colab/pix2pixHD\n","Collecting dominate\n","  Downloading https://files.pythonhosted.org/packages/ef/a8/4354f8122c39e35516a2708746d89db5e339c867abbd8e0179bccee4b7f9/dominate-2.6.0-py2.py3-none-any.whl\n","Installing collected packages: dominate\n","Successfully installed dominate-2.6.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nzLGf05WXMFV"},"source":["## Extract frames from video\n","\n","Upload a video to either Colab or Google Drive.\n","\n","* `-video` is the path to the video file\n","* `-name` should be a unique name (think of it like a project name)\n","* `-width` and `-height` **must** to be a multiple of 32\n","* `-p2pdir` leave as `.` unless you know it shouldn’t be that ;)\n"]},{"cell_type":"code","metadata":{"id":"lWRL2ty6N9LD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623653968846,"user_tz":-480,"elapsed":6021,"user":{"displayName":"Theaveas So","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN_h8gjOMXrtWaxBWxEbW87dJL1IF6Rtq-iYc6=s64","userId":"12302225738999643817"}},"outputId":"36d02e08-c803-4103-d439-e03c8e1b2c3e"},"source":["!python extract_frames.py -video /content/Datasets/landscape.mp4 -name landscape -p2pdir . -width 1280 -height 736"],"execution_count":null,"outputs":[{"output_type":"stream","text":["creating the dataset structure\n","ffmpeg -v 16 -i /content/Datasets/landscape.mp4 -q:v 2 -vf \"scale=iw*736/ih:736, crop=1280:736\" /content/drive/My\\ Drive/nfp-colab/pix2pixHD/datasets/landscape/train_frames/frame-%06d.jpg -hide_banner\n","extracting the frames\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qL9BZkBA_QRR"},"source":["## Train your model"]},{"cell_type":"markdown","metadata":{"id":"4X7qahzMX05u"},"source":["### Initial training\n","\n","Note: if you have a large dataset, this may timeout initially (`ValueError: __len__() should return >= 0`). Give it a minute or two and run it again.\n","\n","*   `--name` should be a unique name (think of it like a project name). For your sanity I recommend using the same `-name` as above.\n","*   `--dataroot` should point to your dataset. This will usually be in `./datasets/[your project name]`\n","\n"," \n","\n"]},{"cell_type":"code","metadata":{"id":"fzHBGVnUKEzE","colab":{"base_uri":"https://localhost:8080/"},"outputId":"05894cbb-8997-405f-ebbf-1f9759e26988"},"source":["!python train_video.py --name landscape --dataroot ./datasets/landscape/ --save_epoch_freq 5 --continue_train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["------------ Options -------------\n","batchSize: 1\n","beta1: 0.5\n","checkpoints_dir: ./checkpoints\n","continue_train: True\n","data_type: 32\n","dataroot: ./datasets/landscape/\n","debug: False\n","display_freq: 100\n","display_winsize: 512\n","feat_num: 3\n","fineSize: 512\n","fp16: False\n","fps: 24.0\n","gpu_ids: [0]\n","heat_seeking_lvl: 0\n","input_nc: 3\n","instance_feat: False\n","isTrain: True\n","label_feat: False\n","label_nc: 35\n","lambda_feat: 10.0\n","loadSize: 1024\n","load_features: False\n","load_pretrain: \n","local_rank: 0\n","lr: 0.0002\n","max_dataset_size: inf\n","model: pix2pixHD\n","nThreads: 2\n","n_blocks_global: 9\n","n_blocks_local: 3\n","n_clusters: 10\n","n_downsample_E: 4\n","n_downsample_global: 4\n","n_layers_D: 3\n","n_local_enhancers: 1\n","name: landscape\n","ndf: 64\n","nef: 16\n","netG: global\n","ngf: 64\n","niter: 100\n","niter_decay: 100\n","niter_fix_global: 0\n","no_flip: False\n","no_ganFeat_loss: False\n","no_html: False\n","no_instance: False\n","no_lsgan: False\n","no_vgg_loss: False\n","norm: instance\n","num_D: 2\n","output_nc: 3\n","phase: train\n","pool_size: 0\n","print_freq: 100\n","pstart: 1\n","pstop: 1\n","resize_or_crop: scale_width\n","save_epoch_freq: 5\n","save_latest_freq: 1000\n","scheduled_sampling: False\n","serial_batches: False\n","ss_recursion_prob: 0.2\n","start_frames_before: 1\n","start_from: video\n","tf_log: False\n","use_dropout: False\n","verbose: False\n","video_mode: False\n","which_epoch: latest\n","zoom_cres: False\n","zoom_inc: 24\n","zoom_lvl: 0\n","-------------- End ----------------\n","Resuming from epoch 11 at iteration 0\n","train_video.py:11: DeprecationWarning: fractions.gcd() is deprecated. Use math.gcd() instead.\n","  def lcm(a,b): return abs(a * b)/fractions.gcd(a,b) if a and b else 0\n","CustomDatasetDataLoader\n","dataset [FrameDataset] was created\n","FrameDataset initialized from: ./datasets/landscape/train_frames\n","contains 126 frames, 125 consecutive pairs\n","#training images = 125\n","\n","create web directory ./checkpoints/landscape/web...\n","(epoch: 11, iters: 100, time: 2.500) G_GAN: 0.780 G_GAN_Feat: 5.722 G_VGG: 3.885 D_real: 0.386 D_fake: 0.528 \n","End of epoch 11 / 200 \t Time Taken: 312 sec\n","(epoch: 12, iters: 75, time: 2.498) G_GAN: 0.809 G_GAN_Feat: 9.571 G_VGG: 4.682 D_real: 0.220 D_fake: 0.413 \n","End of epoch 12 / 200 \t Time Taken: 312 sec\n","(epoch: 13, iters: 50, time: 2.496) G_GAN: 0.921 G_GAN_Feat: 8.130 G_VGG: 4.091 D_real: 0.340 D_fake: 0.447 \n","End of epoch 13 / 200 \t Time Taken: 311 sec\n","(epoch: 14, iters: 25, time: 2.495) G_GAN: 0.537 G_GAN_Feat: 9.247 G_VGG: 4.301 D_real: 0.209 D_fake: 0.810 \n","(epoch: 14, iters: 125, time: 2.495) G_GAN: 1.978 G_GAN_Feat: 9.230 G_VGG: 4.453 D_real: 0.912 D_fake: 0.078 \n","End of epoch 14 / 200 \t Time Taken: 312 sec\n","(epoch: 15, iters: 100, time: 2.497) G_GAN: 0.880 G_GAN_Feat: 9.954 G_VGG: 4.278 D_real: 0.392 D_fake: 0.448 \n","End of epoch 15 / 200 \t Time Taken: 311 sec\n","saving the model at the end of epoch 15, iters 1875\n","(epoch: 16, iters: 75, time: 2.580) G_GAN: 0.719 G_GAN_Feat: 5.573 G_VGG: 3.738 D_real: 0.301 D_fake: 0.375 \n","End of epoch 16 / 200 \t Time Taken: 312 sec\n","(epoch: 17, iters: 50, time: 2.496) G_GAN: 0.960 G_GAN_Feat: 5.540 G_VGG: 3.683 D_real: 0.476 D_fake: 0.285 \n","End of epoch 17 / 200 \t Time Taken: 311 sec\n","(epoch: 18, iters: 25, time: 2.496) G_GAN: 0.497 G_GAN_Feat: 9.517 G_VGG: 4.230 D_real: 0.169 D_fake: 0.740 \n","(epoch: 18, iters: 125, time: 2.496) G_GAN: 0.466 G_GAN_Feat: 7.202 G_VGG: 3.997 D_real: 0.122 D_fake: 0.589 \n","saving the latest model (epoch 18, total_steps 2250)\n","End of epoch 18 / 200 \t Time Taken: 317 sec\n","(epoch: 19, iters: 100, time: 2.497) G_GAN: 0.789 G_GAN_Feat: 8.952 G_VGG: 4.093 D_real: 0.307 D_fake: 0.359 \n","End of epoch 19 / 200 \t Time Taken: 311 sec\n","(epoch: 20, iters: 75, time: 2.496) G_GAN: 1.415 G_GAN_Feat: 6.176 G_VGG: 3.653 D_real: 1.107 D_fake: 0.129 \n","End of epoch 20 / 200 \t Time Taken: 312 sec\n","saving the model at the end of epoch 20, iters 2500\n","(epoch: 21, iters: 50, time: 2.623) G_GAN: 1.059 G_GAN_Feat: 6.473 G_VGG: 3.629 D_real: 0.517 D_fake: 0.257 \n","End of epoch 21 / 200 \t Time Taken: 312 sec\n","(epoch: 22, iters: 25, time: 2.495) G_GAN: 1.491 G_GAN_Feat: 8.231 G_VGG: 3.993 D_real: 0.355 D_fake: 0.475 \n","(epoch: 22, iters: 125, time: 2.495) G_GAN: 0.636 G_GAN_Feat: 9.413 G_VGG: 4.216 D_real: 0.240 D_fake: 0.674 \n","End of epoch 22 / 200 \t Time Taken: 312 sec\n","(epoch: 23, iters: 100, time: 2.495) G_GAN: 0.892 G_GAN_Feat: 8.807 G_VGG: 4.106 D_real: 0.505 D_fake: 0.481 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0GDg3CeW_1TD"},"source":["### Continue Training\n","To pick up from a previous training session run the same command you ran to start with and append `--continue_train` to the end of the command."]},{"cell_type":"code","metadata":{"id":"F5q3dE9S_5eg"},"source":["!python train_video.py --name cuttlefish1 --dataroot ./datasets/cuttlefish1/ --save_epoch_freq 1 --continue_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jly_3OyBoGg2"},"source":["#Generating Videos\n","\n","To generate a video from your completed model, run the `generate_video.py` script. I recommend keeping the `--name` and `--dataroot` arguments the same.\n","\n","There are a number of additional arguments you’ll need to include in this command:\n","\n","\n","*   `--fps` frame rate for your video\n","*   `--how_many` how many frames you want to create (this + fps = video length)\n","*   `--which_epoch` which epoch you want to generate videos from (note: if you choose `133` but there’s no epoch 133 model file, this will throw an error)\n","*   `--start_from` by default your video will start predicting images from the 60th frame of your training set. You can pass in the path to a different frame to have it start from that frame\n","\n","I recommend playing with both the `--which_epoch` and `--start_from` arguments as you can get dramatically different results by changing in the inputs here.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"INVUtG-pt_F6"},"source":["!python generate_video.py --name cuttlefish1 --dataroot ./datasets/cuttlefish1/ --fps 24 --how_many 600 --which_epoch latest "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiaLMDmdDG3t"},"source":["import os\n","\n","def processFolder(folder, epoch = 10, frameCount = 240, skipCount = 1):\n","  files = os.listdir(folder)\n","\n","  count = 0\n","  for f in files:\n","    \n","    if (count % skipCount == 0):\n","      print(f)\n","      if epoch == 'latest':\n","        command = 'python generate_video.py --name glitch_white_circle --dataroot ./datasets/glitch-circle-white_dataset/ --fps 60 --how_many %i --which_epoch latest --start_from %s/%s' % ( frameCount, folder, f)\n","      else:\n","        command = 'python generate_video.py --name glitch_white_circle --dataroot ./datasets/glitch-circle-white_dataset/ --fps 24 --how_many %i --which_epoch %i --start_from %s/%s' % ( frameCount, epoch, folder, f)\n","      os.system(command)\n","    count += 1\n","\n","processFolder('./datasets/fuck-white/train_frames',1,600,3050)"],"execution_count":null,"outputs":[]}]}